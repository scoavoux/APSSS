


Li, Chang, 2015 

Notes de lecture
[PB, 27/11/2019]
L'article porte sur 67 articles parus dans 13 "weel-regarded economics journal" qui permettent en théorie la réplication, c'est-à-dire fournissent script et données (que ce soit ou non une exigence de la revue concernée).
Les auteurs expliquent qu'ils ont obtenu les données et le code d'une majorité de papiers (29 sur 35) pour lesquels la revue exige le code de replication et les données.  En tentant de répliquer, ils arrivent à le faire par eux-mêmes (i.e. sans contacter un auteur) pour seulement un tiers des articles. Certains papiers utilisent des données non-diffusables ("confidential") et quelques autres utilisent un logiciel difficile d'accès. Avec l'assistance des auteurs, c'est possible de répliquer la moitié des articles. Donc la moitié des articles sont réplicables, mais l'autre moitié est non-réplicable.
L'article conclut avec des recommandations pour changer cet état de fait, parmi lesquelles (relativement évidentes) :
-"mandatory data and code files should be a condition of publication"
-l'importance des "readme" files qui devraient mieux indiquer la marche à suivre : temps pris par la procédure, ordre à suivre
-fournir les données originales comme celles retravaillée. 
Commentaire général : l'article n'est pas très original dans la mesure où il s'inscrit en fait dans une longue ligée de papiers de ce type (cités en introduction), qui montrent tous des problèmes similaires, mais dans des champs variés de l'économie.  Son apport est de traiter de plusieurs revues majeures en économie, et de bien montrer le problème, honteux ...

Bernardi, Chakhaia, Leopold, ESR, 2017

Notes de lecture
[PB, 28/11/2019]
Cet article analyse le "ritual of null hypothesis significance testing (NHST)" dans European Sociologcial Review, soit la revue sociologique mainstream (revue de l'ECSR) à l'échelle européenne, en s'interrogeant sur la distinction entre "statistical significance" et "substantive, sociological significance". L'idée est de montrer que les articles portent trop d'attention à la significativité statistique et pas assez à la significativité sociologique - ce qui est un peu facile mais soit. 
Pour cela, il s'appuie sur N=356 articles parus dans la revue entre 2000-2004 puis 2010-2014 (afin de voir si évolution) qui ont été codés avec une grille comprenant 5 points principaux s'inspirant des travaux de Ziliak & McCloskey :
1/ "Do the authors avoid interpreting a statistically insignificant coefficient as evidence of no effect ?"
2/ "Do the authors avoid using the adjective significant in a an ambiguous way ? "
3/ "Do the authors avoid justifying the inclusion of variables in their models on the basis of the statistical significance of their estimates ?"
4/ "Do the authors report coefficients in some useful and intelligible form that makes it easier to understand how large the effect is ?"
5/ "Do the authors discuss the substantive significance of the model coefficients ?"

        Les auteurs relèvent différents types d'erreurs :
1/ confusion significativité statistique / sociologique
2/ insistance sur des résultats statistiquement significatifs mais peu importants
3/ non-significativité statistique interprétée comme "zero-effect"
 
Puis ils trouvent que de larges parts des articles (au moins un tiers pour les différentes fautes qu'ils distinguent) de la revue sont touchés par ces pbs, durant les deux périodes (donc pas d'amélioration).
 
Limite 1 du papier : ils montrent la distribution des articles recensés sur les deux périodes en table 1 p. 3, on voit que la part d'articles sélectionnés, c'est-à-dire qui contiennent concrètement un test statistique et concrètement un modèle de régression, augmentent au cours de la période. Or c'est sans doute ça le résultat central de leur relevé : les articles de la revue recourent de plus en plus aux régressions ! Du coup ils font les autruches sur cet essor de la régression.
Limite 2 du papier : ils font les donneurs de leçon, bien qu'ils s'en défendent (Bernardi est président de l'ECSR et du département socio de l'EUI) en disant que leurs propres travaux sont concernés par ces pbs, mais du coup ils se défilent sur les logiques carriéristes qui sous-tendent leurs propres comportements et sur lesquels ont peut penser que s'appuie cette logique de publications de régressions à tout-va ...
 
Commentaire général : c'est un papier très important parce qu'il sort dans l'ESR et parce qu'il est signé Bernardi, soit autant de caractéristiques mainstream, et en même temps on ne voit pas bien comment ce genre de papier peut faire évoluer les choses. Ils donnent des pseudo-conseils à la fin, d'être plus nuancés dans l'interprétation des résultats etc mais ça reste cheap par rapport à l'enjeu qui est en fait d'ouvrir à d'autres méthodes stat les publications de l'ESR, autrement dit d'aller vers une science plus ouverte, moins routinisée, moins marquée par des enjeux de carrière, avec moins de publications de tous, etc. Autrement dit, ils ne font vraiment qu'un bout de la réflexion sur cet état de fait.

McCloskey, Ziliak, 1996 
Notes de lecture
[PB, 26/11/2019]
Premier papier séminal dans cette veine de recension d'articles et de critique de leur usage des tests stat en économie (parution 1996), même si ça renvoie aux travaux plus anciens de McCloskey dès les années 1980.  Les auteurs établissent une série de questions à poser à chaque article pour qualifier son usage des tests.  Pour cela ils utilisent les articles concernés par cette question et parus dans l'AER 1980-1989, soit environ 180. Ils montrent des taux d'erreurs très important (tableau 1 p. 105).
 
McCloskey, Ziliak, 2004 
Notes de lecture
[PB, 26/11/2019]
Article de Ziliak & McCloskey qui reprend la même démarche qu'en 1996 pour voir si du neuf depuis lors dans l'usage des stat. 
Montrent que cela empire (en tout cas pour pas mal de types d'erreurs, tablea 2 p. 532), et soulignent que cela a des effets sur les propositions de politiques publiques que font les articles ...
 


